{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "COUNTRIES = [\"Afghanistan\",\n",
    "    \"Åland Islands\",\n",
    "    \"Albania\",\n",
    "    \"Algeria\",\n",
    "    \"American Samoa\",\n",
    "    \"Andorra\",\n",
    "    \"Angola\",\n",
    "    \"Anguilla\",\n",
    "    \"Antarctica\",\n",
    "    \"Antigua and Barbuda\",\n",
    "    \"Argentina\",\n",
    "    \"Armenia\",\n",
    "    \"Aruba\",\n",
    "    \"Australia\",\n",
    "    \"Austria\",\n",
    "    \"Azerbaijan\",\n",
    "    \"Bahamas\",\n",
    "    \"Bahrain\",\n",
    "    \"Bangladesh\",\n",
    "    \"Barbados\",\n",
    "    \"Belarus\",\n",
    "    \"Belgium\",\n",
    "    \"Belize\",\n",
    "    \"Benin\",\n",
    "    \"Bermuda\",\n",
    "    \"Bhutan\",\n",
    "    \"Bolivia (Plurinational State of)\",\n",
    "    \"Bonaire, Sint Eustatius and Saba\",\n",
    "    \"Bosnia and Herzegovina\",\n",
    "    \"Botswana\",\n",
    "    \"Bouvet Island\",\n",
    "    \"Brazil\",\n",
    "    \"British Indian Ocean Territory\",\n",
    "    \"United States Minor Outlying Islands\",\n",
    "    \"Virgin Islands (British)\",\n",
    "    \"Virgin Islands (U.S.)\",\n",
    "    \"Brunei Darussalam\",\n",
    "    \"Bulgaria\",\n",
    "    \"Burkina Faso\",\n",
    "    \"Burundi\",\n",
    "    \"Cambodia\",\n",
    "    \"Cameroon\",\n",
    "    \"Canada\",\n",
    "    \"Cabo Verde\",\n",
    "    \"Cayman Islands\",\n",
    "    \"Central African Republic\",\n",
    "    \"Chad\",\n",
    "    \"Chile\",\n",
    "    \"China\",\n",
    "    \"Christmas Island\",\n",
    "    \"Cocos (Keeling) Islands\",\n",
    "    \"Colombia\",\n",
    "    \"Comoros\",\n",
    "    \"Congo\",\n",
    "    \"Congo (Democratic Republic of the)\",\n",
    "    \"Cook Islands\",\n",
    "    \"Costa Rica\",\n",
    "    \"Croatia\",\n",
    "    \"Cuba\",\n",
    "    \"Curaçao\",\n",
    "    \"Cyprus\",\n",
    "    \"Czech Republic\",\n",
    "    \"Denmark\",\n",
    "    \"Djibouti\",\n",
    "    \"Dominica\",\n",
    "    \"Dominican Republic\",\n",
    "    \"Ecuador\",\n",
    "    \"Egypt\",\n",
    "    \"El Salvador\",\n",
    "    \"Equatorial Guinea\",\n",
    "    \"Eritrea\",\n",
    "    \"Estonia\",\n",
    "    \"Ethiopia\",\n",
    "    \"Falkland Islands (Malvinas)\",\n",
    "    \"Faroe Islands\",\n",
    "    \"Fiji\",\n",
    "    \"Finland\",\n",
    "    \"France\",\n",
    "    \"French Guiana\",\n",
    "    \"French Polynesia\",\n",
    "    \"French Southern Territories\",\n",
    "    \"Gabon\",\n",
    "    \"Gambia\",\n",
    "    \"Georgia\",\n",
    "    \"Germany\",\n",
    "    \"Ghana\",\n",
    "    \"Gibraltar\",\n",
    "    \"Greece\",\n",
    "    \"Greenland\",\n",
    "    \"Grenada\",\n",
    "    \"Guadeloupe\",\n",
    "    \"Guam\",\n",
    "    \"Guatemala\",\n",
    "    \"Guernsey\",\n",
    "    \"Guinea\",\n",
    "    \"Guinea-Bissau\",\n",
    "    \"Guyana\",\n",
    "    \"Haiti\",\n",
    "    \"Heard Island and McDonald Islands\",\n",
    "    \"Holy See\",\n",
    "    \"Honduras\",\n",
    "    \"Hong Kong\",\n",
    "    \"Hungary\",\n",
    "    \"Iceland\",\n",
    "    \"India\",\n",
    "    \"Indonesia\",\n",
    "    \"Côte d'Ivoire\",\n",
    "    \"Iran (Islamic Republic of)\",\n",
    "    \"Iraq\",\n",
    "    \"Ireland\",\n",
    "    \"Isle of Man\",\n",
    "    \"Israel\",\n",
    "    \"Italy\",\n",
    "    \"Jamaica\",\n",
    "    \"Japan\",\n",
    "    \"Jersey\",\n",
    "    \"Jordan\",\n",
    "    \"Kazakhstan\",\n",
    "    \"Kenya\",\n",
    "    \"Kiribati\",\n",
    "    \"Kuwait\",\n",
    "    \"Kyrgyzstan\",\n",
    "    \"Lao People's Democratic Republic\",\n",
    "    \"Latvia\",\n",
    "    \"Lebanon\",\n",
    "    \"Lesotho\",\n",
    "    \"Liberia\",\n",
    "    \"Libya\",\n",
    "    \"Liechtenstein\",\n",
    "    \"Lithuania\",\n",
    "    \"Luxembourg\",\n",
    "    \"Macao\",\n",
    "    \"Macedonia (the former Yugoslav Republic of)\",\n",
    "    \"Madagascar\",\n",
    "    \"Malawi\",\n",
    "    \"Malaysia\",\n",
    "    \"Maldives\",\n",
    "    \"Mali\",\n",
    "    \"Malta\",\n",
    "    \"Marshall Islands\",\n",
    "    \"Martinique\",\n",
    "    \"Mauritania\",\n",
    "    \"Mauritius\",\n",
    "    \"Mayotte\",\n",
    "    \"Mexico\",\n",
    "    \"Micronesia (Federated States of)\",\n",
    "    \"Moldova (Republic of)\",\n",
    "    \"Monaco\",\n",
    "    \"Mongolia\",\n",
    "    \"Montenegro\",\n",
    "    \"Montserrat\",\n",
    "    \"Morocco\",\n",
    "    \"Mozambique\",\n",
    "    \"Myanmar\",\n",
    "    \"Namibia\",\n",
    "    \"Nauru\",\n",
    "    \"Nepal\",\n",
    "    \"Netherlands\",\n",
    "    \"New Caledonia\",\n",
    "    \"New Zealand\",\n",
    "    \"Nicaragua\",\n",
    "    \"Niger\",\n",
    "    \"Nigeria\",\n",
    "    \"Niue\",\n",
    "    \"Norfolk Island\",\n",
    "    \"Korea (Democratic People's Republic of)\",\n",
    "    \"Northern Mariana Islands\",\n",
    "    \"Norway\",\n",
    "    \"Oman\",\n",
    "    \"Pakistan\",\n",
    "    \"Palau\",\n",
    "    \"Palestine, State of\",\n",
    "    \"Panama\",\n",
    "    \"Papua New Guinea\",\n",
    "    \"Paraguay\",\n",
    "    \"Peru\",\n",
    "    \"Philippines\",\n",
    "    \"Pitcairn\",\n",
    "    \"Poland\",\n",
    "    \"Portugal\",\n",
    "    \"Puerto Rico\",\n",
    "    \"Qatar\",\n",
    "    \"Republic of Kosovo\",\n",
    "    \"Réunion\",\n",
    "    \"Romania\",\n",
    "    \"Russian Federation\",\n",
    "    \"Rwanda\",\n",
    "    \"Saint Barthélemy\",\n",
    "    \"Saint Helena, Ascension and Tristan da Cunha\",\n",
    "    \"Saint Kitts and Nevis\",\n",
    "    \"Saint Lucia\",\n",
    "    \"Saint Martin (French part)\",\n",
    "    \"Saint Pierre and Miquelon\",\n",
    "    \"Saint Vincent and the Grenadines\",\n",
    "    \"Samoa\",\n",
    "    \"San Marino\",\n",
    "    \"Sao Tome and Principe\",\n",
    "    \"Saudi Arabia\",\n",
    "    \"Senegal\",\n",
    "    \"Serbia\",\n",
    "    \"Seychelles\",\n",
    "    \"Sierra Leone\",\n",
    "    \"Singapore\",\n",
    "    \"Sint Maarten (Dutch part)\",\n",
    "    \"Slovakia\",\n",
    "    \"Slovenia\",\n",
    "    \"Solomon Islands\",\n",
    "    \"Somalia\",\n",
    "    \"South Africa\",\n",
    "    \"South Georgia and the South Sandwich Islands\",\n",
    "    \"Korea (Republic of)\",\n",
    "    \"South Sudan\",\n",
    "    \"Spain\",\n",
    "    \"Sri Lanka\",\n",
    "    \"Sudan\",\n",
    "    \"Suriname\",\n",
    "    \"Svalbard and Jan Mayen\",\n",
    "    \"Swaziland\",\n",
    "    \"Sweden\",\n",
    "    \"Switzerland\",\n",
    "    \"Syrian Arab Republic\",\n",
    "    \"Taiwan\",\n",
    "    \"Tajikistan\",\n",
    "    \"Tanzania, United Republic of\",\n",
    "    \"Thailand\",\n",
    "    \"Timor-Leste\",\n",
    "    \"Togo\",\n",
    "    \"Tokelau\",\n",
    "    \"Tonga\",\n",
    "    \"Trinidad and Tobago\",\n",
    "    \"Tunisia\",\n",
    "    \"Turkey\",\n",
    "    \"Turkmenistan\",\n",
    "    \"Turks and Caicos Islands\",\n",
    "    \"Tuvalu\",\n",
    "    \"Uganda\",\n",
    "    \"Ukraine\",\n",
    "    \"United Arab Emirates\",\n",
    "    \"United Kingdom of Great Britain and Northern Ireland\",\n",
    "    \"United States of America\",\n",
    "    \"Uruguay\",\n",
    "    \"Uzbekistan\",\n",
    "    \"Vanuatu\",\n",
    "    \"Venezuela (Bolivarian Republic of)\",\n",
    "    \"Viet Nam\",\n",
    "    \"Wallis and Futuna\",\n",
    "    \"Western Sahara\",\n",
    "    \"Yemen\",\n",
    "    \"Zambia\",\n",
    "    \"Zimbabwe\"]\n",
    "\n",
    "CAPITALS = {\n",
    "  \"Afghanistan\":\"Kabul\",\n",
    "  \"\\u00c5land Islands\":\"Mariehamn\",\n",
    "  \"Albania\":\"Tirana\",\n",
    "  \"Algeria\":\"Algiers\",\n",
    "  \"American Samoa\":\"Pago Pago\",\n",
    "  \"Andorra\":\"Andorra la Vella\",\n",
    "  \"Angola\":\"Luanda\",\n",
    "  \"Anguilla\":\"The Valley\",\n",
    "  \"Antarctica\":\"\",\n",
    "  \"Antigua and Barbuda\":\"Saint John's\",\n",
    "  \"Argentina\":\"Buenos Aires\",\n",
    "  \"Armenia\":\"Yerevan\",\n",
    "  \"Aruba\":\"Oranjestad\",\n",
    "  \"Australia\":\"Canberra\",\n",
    "  \"Austria\":\"Vienna\",\n",
    "  \"Azerbaijan\":\"Baku\",\n",
    "  \"Bahamas\":\"Nassau\",\n",
    "  \"Bahrain\":\"Manama\",\n",
    "  \"Bangladesh\":\"Dhaka\",\n",
    "  \"Barbados\":\"Bridgetown\",\n",
    "  \"Belarus\":\"Minsk\",\n",
    "  \"Belgium\":\"Brussels\",\n",
    "  \"Belize\":\"Belmopan\",\n",
    "  \"Benin\":\"Porto-Novo\",\n",
    "  \"Bermuda\":\"Hamilton\",\n",
    "  \"Bhutan\":\"Thimphu\",\n",
    "  \"Bolivia (Plurinational State of)\":\"Sucre\",\n",
    "  \"Bonaire, Sint Eustatius and Saba\":\"Kralendijk\",\n",
    "  \"Bosnia and Herzegovina\":\"Sarajevo\",\n",
    "  \"Botswana\":\"Gaborone\",\n",
    "  \"Bouvet Island\":\"\",\n",
    "  \"Brazil\":\"Bras\\u00edlia\",\n",
    "  \"British Indian Ocean Territory\":\"Diego Garcia\",\n",
    "  \"United States Minor Outlying Islands\":\"\",\n",
    "  \"Virgin Islands (British)\":\"Road Town\",\n",
    "  \"Virgin Islands (U.S.)\":\"Charlotte Amalie\",\n",
    "  \"Brunei Darussalam\":\"Bandar Seri Begawan\",\n",
    "  \"Bulgaria\":\"Sofia\",\n",
    "  \"Burkina Faso\":\"Ouagadougou\",\n",
    "  \"Burundi\":\"Bujumbura\",\n",
    "  \"Cambodia\":\"Phnom Penh\",\n",
    "  \"Cameroon\":\"Yaound\\u00e9\",\n",
    "  \"Canada\":\"Ottawa\",\n",
    "  \"Cabo Verde\":\"Praia\",\n",
    "  \"Cayman Islands\":\"George Town\",\n",
    "  \"Central African Republic\":\"Bangui\",\n",
    "  \"Chad\":\"N'Djamena\",\n",
    "  \"Chile\":\"Santiago\",\n",
    "  \"China\":\"Beijing\",\n",
    "  \"Christmas Island\":\"Flying Fish Cove\",\n",
    "  \"Cocos (Keeling) Islands\":\"West Island\",\n",
    "  \"Colombia\":\"Bogot\\u00e1\",\n",
    "  \"Comoros\":\"Moroni\",\n",
    "  \"Congo\":\"Brazzaville\",\n",
    "  \"Congo (Democratic Republic of the)\":\"Kinshasa\",\n",
    "  \"Cook Islands\":\"Avarua\",\n",
    "  \"Costa Rica\":\"San Jos\\u00e9\",\n",
    "  \"Croatia\":\"Zagreb\",\n",
    "  \"Cuba\":\"Havana\",\n",
    "  \"Cura\\u00e7ao\":\"Willemstad\",\n",
    "  \"Cyprus\":\"Nicosia\",\n",
    "  \"Czech Republic\":\"Prague\",\n",
    "  \"Denmark\":\"Copenhagen\",\n",
    "  \"Djibouti\":\"Djibouti\",\n",
    "  \"Dominica\":\"Roseau\",\n",
    "  \"Dominican Republic\":\"Santo Domingo\",\n",
    "  \"Ecuador\":\"Quito\",\n",
    "  \"Egypt\":\"Cairo\",\n",
    "  \"El Salvador\":\"San Salvador\",\n",
    "  \"Equatorial Guinea\":\"Malabo\",\n",
    "  \"Eritrea\":\"Asmara\",\n",
    "  \"Estonia\":\"Tallinn\",\n",
    "  \"Ethiopia\":\"Addis Ababa\",\n",
    "  \"Falkland Islands (Malvinas)\":\"Stanley\",\n",
    "  \"Faroe Islands\":\"T\\u00f3rshavn\",\n",
    "  \"Fiji\":\"Suva\",\n",
    "  \"Finland\":\"Helsinki\",\n",
    "  \"France\":\"Paris\",\n",
    "  \"French Guiana\":\"Cayenne\",\n",
    "  \"French Polynesia\":\"Papeet\\u0113\",\n",
    "  \"French Southern Territories\":\"Port-aux-Fran\\u00e7ais\",\n",
    "  \"Gabon\":\"Libreville\",\n",
    "  \"Gambia\":\"Banjul\",\n",
    "  \"Georgia\":\"Tbilisi\",\n",
    "  \"Germany\":\"Berlin\",\n",
    "  \"Ghana\":\"Accra\",\n",
    "  \"Gibraltar\":\"Gibraltar\",\n",
    "  \"Greece\":\"Athens\",\n",
    "  \"Greenland\":\"Nuuk\",\n",
    "  \"Grenada\":\"St. George's\",\n",
    "  \"Guadeloupe\":\"Basse-Terre\",\n",
    "  \"Guam\":\"Hag\\u00e5t\\u00f1a\",\n",
    "  \"Guatemala\":\"Guatemala City\",\n",
    "  \"Guernsey\":\"St. Peter Port\",\n",
    "  \"Guinea\":\"Conakry\",\n",
    "  \"Guinea-Bissau\":\"Bissau\",\n",
    "  \"Guyana\":\"Georgetown\",\n",
    "  \"Haiti\":\"Port-au-Prince\",\n",
    "  \"Heard Island and McDonald Islands\":\"\",\n",
    "  \"Holy See\":\"Rome\",\n",
    "  \"Honduras\":\"Tegucigalpa\",\n",
    "  \"Hong Kong\":\"City of Victoria\",\n",
    "  \"Hungary\":\"Budapest\",\n",
    "  \"Iceland\":\"Reykjav\\u00edk\",\n",
    "  \"India\":\"New Delhi\",\n",
    "  \"Indonesia\":\"Jakarta\",\n",
    "  \"C\\u00f4te d'Ivoire\":\"Yamoussoukro\",\n",
    "  \"Iran (Islamic Republic of)\":\"Tehran\",\n",
    "  \"Iraq\":\"Baghdad\",\n",
    "  \"Ireland\":\"Dublin\",\n",
    "  \"Isle of Man\":\"Douglas\",\n",
    "  \"Israel\":\"Jerusalem\",\n",
    "  \"Italy\":\"Rome\",\n",
    "  \"Jamaica\":\"Kingston\",\n",
    "  \"Japan\":\"Tokyo\",\n",
    "  \"Jersey\":\"Saint Helier\",\n",
    "  \"Jordan\":\"Amman\",\n",
    "  \"Kazakhstan\":\"Astana\",\n",
    "  \"Kenya\":\"Nairobi\",\n",
    "  \"Kiribati\":\"South Tarawa\",\n",
    "  \"Kuwait\":\"Kuwait City\",\n",
    "  \"Kyrgyzstan\":\"Bishkek\",\n",
    "  \"Lao People's Democratic Republic\":\"Vientiane\",\n",
    "  \"Latvia\":\"Riga\",\n",
    "  \"Lebanon\":\"Beirut\",\n",
    "  \"Lesotho\":\"Maseru\",\n",
    "  \"Liberia\":\"Monrovia\",\n",
    "  \"Libya\":\"Tripoli\",\n",
    "  \"Liechtenstein\":\"Vaduz\",\n",
    "  \"Lithuania\":\"Vilnius\",\n",
    "  \"Luxembourg\":\"Luxembourg\",\n",
    "  \"Macao\":\"\",\n",
    "  \"Macedonia (the former Yugoslav Republic of)\":\"Skopje\",\n",
    "  \"Madagascar\":\"Antananarivo\",\n",
    "  \"Malawi\":\"Lilongwe\",\n",
    "  \"Malaysia\":\"Kuala Lumpur\",\n",
    "  \"Maldives\":\"Mal\\u00e9\",\n",
    "  \"Mali\":\"Bamako\",\n",
    "  \"Malta\":\"Valletta\",\n",
    "  \"Marshall Islands\":\"Majuro\",\n",
    "  \"Martinique\":\"Fort-de-France\",\n",
    "  \"Mauritania\":\"Nouakchott\",\n",
    "  \"Mauritius\":\"Port Louis\",\n",
    "  \"Mayotte\":\"Mamoudzou\",\n",
    "  \"Mexico\":\"Mexico City\",\n",
    "  \"Micronesia (Federated States of)\":\"Palikir\",\n",
    "  \"Moldova (Republic of)\":\"Chi\\u0219in\\u0103u\",\n",
    "  \"Monaco\":\"Monaco\",\n",
    "  \"Mongolia\":\"Ulan Bator\",\n",
    "  \"Montenegro\":\"Podgorica\",\n",
    "  \"Montserrat\":\"Plymouth\",\n",
    "  \"Morocco\":\"Rabat\",\n",
    "  \"Mozambique\":\"Maputo\",\n",
    "  \"Myanmar\":\"Naypyidaw\",\n",
    "  \"Namibia\":\"Windhoek\",\n",
    "  \"Nauru\":\"Yaren\",\n",
    "  \"Nepal\":\"Kathmandu\",\n",
    "  \"Netherlands\":\"Amsterdam\",\n",
    "  \"New Caledonia\":\"Noum\\u00e9a\",\n",
    "  \"New Zealand\":\"Wellington\",\n",
    "  \"Nicaragua\":\"Managua\",\n",
    "  \"Niger\":\"Niamey\",\n",
    "  \"Nigeria\":\"Abuja\",\n",
    "  \"Niue\":\"Alofi\",\n",
    "  \"Norfolk Island\":\"Kingston\",\n",
    "  \"Korea (Democratic People's Republic of)\":\"Pyongyang\",\n",
    "  \"Northern Mariana Islands\":\"Saipan\",\n",
    "  \"Norway\":\"Oslo\",\n",
    "  \"Oman\":\"Muscat\",\n",
    "  \"Pakistan\":\"Islamabad\",\n",
    "  \"Palau\":\"Ngerulmud\",\n",
    "  \"Palestine, State of\":\"Ramallah\",\n",
    "  \"Panama\":\"Panama City\",\n",
    "  \"Papua New Guinea\":\"Port Moresby\",\n",
    "  \"Paraguay\":\"Asunci\\u00f3n\",\n",
    "  \"Peru\":\"Lima\",\n",
    "  \"Philippines\":\"Manila\",\n",
    "  \"Pitcairn\":\"Adamstown\",\n",
    "  \"Poland\":\"Warsaw\",\n",
    "  \"Portugal\":\"Lisbon\",\n",
    "  \"Puerto Rico\":\"San Juan\",\n",
    "  \"Qatar\":\"Doha\",\n",
    "  \"Republic of Kosovo\":\"Pristina\",\n",
    "  \"R\\u00e9union\":\"Saint-Denis\",\n",
    "  \"Romania\":\"Bucharest\",\n",
    "  \"Russian Federation\":\"Moscow\",\n",
    "  \"Rwanda\":\"Kigali\",\n",
    "  \"Saint Barth\\u00e9lemy\":\"Gustavia\",\n",
    "  \"Saint Helena, Ascension and Tristan da Cunha\":\"Jamestown\",\n",
    "  \"Saint Kitts and Nevis\":\"Basseterre\",\n",
    "  \"Saint Lucia\":\"Castries\",\n",
    "  \"Saint Martin (French part)\":\"Marigot\",\n",
    "  \"Saint Pierre and Miquelon\":\"Saint-Pierre\",\n",
    "  \"Saint Vincent and the Grenadines\":\"Kingstown\",\n",
    "  \"Samoa\":\"Apia\",\n",
    "  \"San Marino\":\"City of San Marino\",\n",
    "  \"Sao Tome and Principe\":\"S\\u00e3o Tom\\u00e9\",\n",
    "  \"Saudi Arabia\":\"Riyadh\",\n",
    "  \"Senegal\":\"Dakar\",\n",
    "  \"Serbia\":\"Belgrade\",\n",
    "  \"Seychelles\":\"Victoria\",\n",
    "  \"Sierra Leone\":\"Freetown\",\n",
    "  \"Singapore\":\"Singapore\",\n",
    "  \"Sint Maarten (Dutch part)\":\"Philipsburg\",\n",
    "  \"Slovakia\":\"Bratislava\",\n",
    "  \"Slovenia\":\"Ljubljana\",\n",
    "  \"Solomon Islands\":\"Honiara\",\n",
    "  \"Somalia\":\"Mogadishu\",\n",
    "  \"South Africa\":\"Pretoria\",\n",
    "  \"South Georgia and the South Sandwich Islands\":\"King Edward Point\",\n",
    "  \"Korea (Republic of)\":\"Seoul\",\n",
    "  \"South Sudan\":\"Juba\",\n",
    "  \"Spain\":\"Madrid\",\n",
    "  \"Sri Lanka\":\"Colombo\",\n",
    "  \"Sudan\":\"Khartoum\",\n",
    "  \"Suriname\":\"Paramaribo\",\n",
    "  \"Svalbard and Jan Mayen\":\"Longyearbyen\",\n",
    "  \"Swaziland\":\"Lobamba\",\n",
    "  \"Sweden\":\"Stockholm\",\n",
    "  \"Switzerland\":\"Bern\",\n",
    "  \"Syrian Arab Republic\":\"Damascus\",\n",
    "  \"Taiwan\":\"Taipei\",\n",
    "  \"Tajikistan\":\"Dushanbe\",\n",
    "  \"Tanzania, United Republic of\":\"Dodoma\",\n",
    "  \"Thailand\":\"Bangkok\",\n",
    "  \"Timor-Leste\":\"Dili\",\n",
    "  \"Togo\":\"Lom\\u00e9\",\n",
    "  \"Tokelau\":\"Fakaofo\",\n",
    "  \"Tonga\":\"Nuku'alofa\",\n",
    "  \"Trinidad and Tobago\":\"Port of Spain\",\n",
    "  \"Tunisia\":\"Tunis\",\n",
    "  \"Turkey\":\"Ankara\",\n",
    "  \"Turkmenistan\":\"Ashgabat\",\n",
    "  \"Turks and Caicos Islands\":\"Cockburn Town\",\n",
    "  \"Tuvalu\":\"Funafuti\",\n",
    "  \"Uganda\":\"Kampala\",\n",
    "  \"Ukraine\":\"Kiev\",\n",
    "  \"United Arab Emirates\":\"Abu Dhabi\",\n",
    "  \"United Kingdom of Great Britain and Northern Ireland\":\"London\",\n",
    "  \"United States of America\":\"Washington, D.C.\",\n",
    "  \"Uruguay\":\"Montevideo\",\n",
    "  \"Uzbekistan\":\"Tashkent\",\n",
    "  \"Vanuatu\":\"Port Vila\",\n",
    "  \"Venezuela (Bolivarian Republic of)\":\"Caracas\",\n",
    "  \"Viet Nam\":\"Hanoi\",\n",
    "  \"Wallis and Futuna\":\"Mata-Utu\",\n",
    "  \"Western Sahara\":\"El Aai\\u00fan\",\n",
    "  \"Yemen\":\"Sana'a\",\n",
    "  \"Zambia\":\"Lusaka\",\n",
    "  \"Zimbabwe\":\"Harare\"\n",
    "}\n",
    "\n",
    "COUNTRY_TEXT = \"\"\"After the Cold War, the UN saw a radical expansion in its peacekeeping duties, taking on more missions in ten years than it had in the previous four decades.Between 1988 and 2000, the number of adopted Security Council resolutions more than doubled, and the peacekeeping budget increased more than tenfold. The UN negotiated an end to the Salvadoran Civil War, launched a successful peacekeeping mission in Namibia, and oversaw democratic elections in post-apartheid South Africa and post-Khmer Rouge Cambodia. In 1991, the UN authorized a US-led coalition that repulsed the Iraqi invasion of Kuwait. Brian Urquhart, Under-Secretary-General from 1971 to 1985, later described the hopes raised by these successes as a \"false renaissance\" for the organization, given the more troubled missions that followed. Though the UN Charter had been written primarily to prevent aggression by one nation against another, in the early 1990s the UN faced a number of simultaneous, serious crises within nations such as Somalia, Haiti, Mozambique, and the former Yugoslavia. The UN mission in Somalia was widely viewed as a failure after the US withdrawal following casualties in the Battle of Mogadishu, and the UN mission to Bosnia faced \"worldwide ridicule\" for its indecisive and confused mission in the face of ethnic cleansing. In 1994, the UN Assistance Mission for Rwanda failed to intervene in the Rwandan genocide amid indecision in the Security Council. Beginning in the last decades of the Cold War, American and European critics of the UN condemned the organization for perceived mismanagement and corruption. In 1984, the US President, Ronald Reagan, withdrew his nation's funding from UNESCO (the United Nations Educational, Scientific and Cultural Organization, founded 1946) over allegations of mismanagement, followed by Britain and Singapore. Boutros Boutros-Ghali, Secretary-General from 1992 to 1996, initiated a reform of the Secretariat, reducing the size of the organization somewhat. His successor, Kofi Annan (1997–2006), initiated further management reforms in the face of threats from the United States to withhold its UN dues. In the late 1990s and 2000s, international interventions authorized by the UN took a wider variety of forms. The UN mission in the Sierra Leone Civil War of 1991–2002 was supplemented by British Royal Marines, and the invasion of Afghanistan in 2001 was overseen by NATO. In 2003, the United States invaded Iraq despite failing to pass a UN Security Council resolution for authorization, prompting a new round of questioning of the organization's effectiveness. Under the eighth Secretary-General, Ban Ki-moon, the UN has intervened with peacekeepers in crises including the War in Darfur in Sudan and the Kivu conflict in the Democratic Republic of Congo and sent observers and chemical weapons inspectors to the Syrian Civil War. In 2013, an internal review of UN actions in the final battles of the Sri Lankan Civil War in 2009 concluded that the organization had suffered \"systemic failure\". One hundred and one UN personnel died in the 2010 Haiti earthquake, the worst loss of life in the organization's history. The Millennium Summit was held in 2000 to discuss the UN's role in the 21st century. The three day meeting was the largest gathering of world leaders in history, and culminated in the adoption by all member states of the Millennium Development Goals (MDGs), a commitment to achieve international development in areas such as poverty reduction, gender equality, and public health. Progress towards these goals, which were to be met by 2015, was ultimately uneven. The 2005 World Summit reaffirmed the UN's focus on promoting development, peacekeeping, human rights, and global security. The Sustainable Development Goals were launched in 2015 to succeed the Millennium Development Goals. In addition to addressing global challenges, the UN has sought to improve its accountability and democratic legitimacy by engaging more with civil society and fostering a global constituency. In an effort to enhance transparency, in 2016 the organization held its first public debate between candidates for Secretary-General. On 1 January 2017, Portuguese diplomat António Guterres, who previously served as UN High Commissioner for Refugees, became the ninth Secretary-General. Guterres has highlighted several key goals for his administration, including an emphasis on diplomacy for preventing conflicts, more effective peacekeeping efforts, and streamlining the organization to be more responsive and versatile to global needs.\"\"\"\n",
    "DATA = [\n",
    "    [\n",
    "        \"One morning, when Gregor Samsa woke from troubled dreams, he found himself transformed in his bed into a horrible vermin.\",\n",
    "        { \"author\": \"Franz Kafka\", \"book\": \"Metamorphosis\" }\n",
    "    ],\n",
    "    [\n",
    "        \"I know not all that may be coming, but be it what it will, I'll go to it laughing.\",\n",
    "        { \"author\": \"Herman Melville\", \"book\": \"Moby-Dick or, The Whale\" }\n",
    "    ],\n",
    "    [\n",
    "        \"It was the best of times, it was the worst of times.\",\n",
    "        { \"author\": \"Charles Dickens\", \"book\": \"A Tale of Two Cities\" }\n",
    "    ],\n",
    "    [\n",
    "        \"The only people for me are the mad ones, the ones who are mad to live, mad to talk, mad to be saved, desirous of everything at the same time, the ones who never yawn or say a commonplace thing, but burn, burn, burn like fabulous yellow roman candles exploding like spiders across the stars.\",\n",
    "        { \"author\": \"Jack Kerouac\", \"book\": \"On the Road\" }\n",
    "    ],\n",
    "    [\n",
    "        \"It was a bright cold day in April, and the clocks were striking thirteen.\",\n",
    "        { \"author\": \"George Orwell\", \"book\": \"1984\" }\n",
    "    ],\n",
    "    [\n",
    "        \"Nowadays people know the price of everything and the value of nothing.\",\n",
    "        { \"author\": \"Oscar Wilde\", \"book\": \"The Picture Of Dorian Gray\" }\n",
    "    ]\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spacy Tutorial:\n",
    "## Chapter 1: Finding words, phrases, names and concept\n",
    "#### Getting Started:\n",
    "\n",
    "- Import the English class from spacy.lang.en and create the nlp object.\n",
    "- Create a doc and print its text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a sentence.\n"
     ]
    }
   ],
   "source": [
    "#Import the English class from spacy.lang.en and create the nlp object.\n",
    "#Create a doc and print its text.\n",
    "\n",
    "# Import the English language class\n",
    "from spacy.lang.en import English\n",
    "\n",
    "# Create the nlp object\n",
    "nlp = English()\n",
    "\n",
    "# Process a text\n",
    "doc = nlp(\"This is a sentence.\")\n",
    "\n",
    "# Print the document text\n",
    "print(doc.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Documents, Spans, and Tokens\n",
    "\n",
    "- Import the English language class and create the nlp object.\n",
    "- Process the text and instantiate a Doc object in the variable doc.\n",
    "- Select the first token of the Doc and print its text.\n",
    "- Create a slice of the Doc for the tokens “tree kangaroos” and “tree kangaroos and narwhals”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I\n"
     ]
    }
   ],
   "source": [
    "# Import the English language class and create the nlp object\n",
    "from spacy.lang.en import English\n",
    "\n",
    "nlp = English()\n",
    "\n",
    "# Process the text\n",
    "doc = nlp(\"I like tree kangaroos and narwhals.\")\n",
    "\n",
    "# Select the first token\n",
    "first_token = doc[0]\n",
    "\n",
    "# Print the first token's text\n",
    "print(first_token.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tree kangaroos\n"
     ]
    }
   ],
   "source": [
    "# A slice of the Doc for \"tree kangaroos\"\n",
    "tree_kangaroos = doc[2:4]\n",
    "print(tree_kangaroos.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tree kangaroos and narwhals\n"
     ]
    }
   ],
   "source": [
    "# A slice of the Doc for \"tree kangaroos and narwhals\" (without the \".\")\n",
    "tree_kangaroos_and_narwhals = doc[2:6]\n",
    "print(tree_kangaroos_and_narwhals.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lexical Attributes\n",
    "\n",
    "- Use the like_num token attribute to check whether a token in the doc resembles a number.\n",
    "- Get the token following the current token in the document. \n",
    "- The index of the next token in the doc is token.i + 1.\n",
    "- Check whether the next token’s text attribute is a percent sign ”%“."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage found: 60\n",
      "Percentage found: 4\n"
     ]
    }
   ],
   "source": [
    "from spacy.lang.en import English\n",
    "\n",
    "nlp = English()\n",
    "\n",
    "# Process the text\n",
    "doc = nlp(\n",
    "    \"In 1990, more than 60% of people in East Asia were in extreme poverty. \"\n",
    "    \"Now less than 4% are.\"\n",
    ")\n",
    "\n",
    "# Iterate over the tokens in the doc\n",
    "for token in doc:\n",
    "    # Check if the token resembles a number\n",
    "    if token.like_num:\n",
    "        # Get the next token in the document\n",
    "        next_token = doc[token.i + 1]\n",
    "        # Check if the next token's text equals '%'\n",
    "        if next_token.text == \"%\":\n",
    "            print(\"Percentage found:\", token.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading Models\n",
    "\n",
    "- Use spacy.load to load the small English model 'en_core_web_sm'.\n",
    "- Process the text and print the document text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It’s official: Apple is the first U.S. public company to reach a $1 trillion market value\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Load the 'en_core_web_sm' model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "text = \"It’s official: Apple is the first U.S. public company to reach a $1 trillion market value\"\n",
    "\n",
    "# Process the text\n",
    "doc = nlp(text)\n",
    "\n",
    "# Print the document text\n",
    "print(doc.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predicting Linguistic Annotations\n",
    "\n",
    "- Process the text with the nlp object and create a doc.\n",
    "- For each token, print the token text, the token’s .pos_ (part-of-speech tag) and the token’s .dep_ (dependency label).\n",
    "- Iterate over the doc.ents and print the entity text and label_ attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It          PRON      nsubj     \n",
      "’s          PROPN     ROOT      \n",
      "official    NOUN      acomp     \n",
      ":           PUNCT     punct     \n",
      "Apple       PROPN     nsubj     \n",
      "is          VERB      ROOT      \n",
      "the         DET       det       \n",
      "first       ADJ       amod      \n",
      "U.S.        PROPN     nmod      \n",
      "public      ADJ       amod      \n",
      "company     NOUN      attr      \n",
      "to          PART      aux       \n",
      "reach       VERB      relcl     \n",
      "a           DET       det       \n",
      "$           SYM       quantmod  \n",
      "1           NUM       compound  \n",
      "trillion    NUM       nummod    \n",
      "market      NOUN      compound  \n",
      "value       NOUN      dobj      \n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "text = \"It’s official: Apple is the first U.S. public company to reach a $1 trillion market value\"\n",
    "\n",
    "# Process the text\n",
    "doc = nlp(text)\n",
    "\n",
    "for token in doc:\n",
    "    # Get the token text, part-of-speech tag and dependency label\n",
    "    token_text = token.text\n",
    "    token_pos = token.pos_\n",
    "    token_dep = token.dep_\n",
    "    # This is for formatting only\n",
    "    print(\"{:<12}{:<10}{:<10}\".format(token_text, token_pos, token_dep))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple ORG\n",
      "first ORDINAL\n",
      "U.S. GPE\n",
      "$1 trillion MONEY\n"
     ]
    }
   ],
   "source": [
    "# Iterate over the predicted entities\n",
    "for ent in doc.ents:\n",
    "    # Print the entity text and its label\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predicting Named Entities in Context\n",
    "\n",
    "- Process the text with the nlp object.\n",
    "- Iterate over the entities and print the entity text and label.\n",
    "- Looks like the model didn’t predict “iPhone X”. Create a span for those tokens manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple ORG\n",
      "Missing entity: iPhone X\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "text = \"New iPhone X release date leaked as Apple reveals pre-orders by mistake\"\n",
    "\n",
    "# Process the text\n",
    "doc = nlp(text)\n",
    "\n",
    "# Iterate over the entities\n",
    "for ent in doc.ents:\n",
    "    # Print the entity text and label\n",
    "    print(ent.text, ent.label_)\n",
    "\n",
    "# Get the span for \"iPhone X\"\n",
    "iphone_x = doc[1:3]\n",
    "\n",
    "# Print the span text\n",
    "print(\"Missing entity:\", iphone_x.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using the Matcher\n",
    "\n",
    "- Import the Matcher from spacy.matcher.\n",
    "- Initialize it with the nlp object’s shared vocab.\n",
    "- Create a pattern that matches the 'TEXT' values of two tokens: \"iPhone\" and \"X\".\n",
    "- Use the matcher.add method to add the pattern to the matcher.\n",
    "- Call the matcher on the doc and store the result in the variable matches.\n",
    "- Iterate over the matches and get the matched span from the start to the end index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matches: ['iPhone X']\n"
     ]
    }
   ],
   "source": [
    "# Import the Matcher\n",
    "from spacy.matcher import Matcher\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\"New iPhone X release date leaked as Apple reveals pre-orders by mistake\")\n",
    "\n",
    "# Initialize the Matcher with the shared vocabulary\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "# Create a pattern matching two tokens: \"iPhone\" and \"X\"\n",
    "pattern = [{\"TEXT\": \"iPhone\"}, {\"TEXT\": \"X\"}]\n",
    "\n",
    "# Add the pattern to the matcher\n",
    "matcher.add(\"IPHONE_X_PATTERN\", None, pattern)\n",
    "\n",
    "# Use the matcher on the doc\n",
    "matches = matcher(doc)\n",
    "print(\"Matches:\", [doc[start:end].text for match_id, start, end in matches])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Writing Match Patterns\n",
    "\n",
    "- Write one pattern that only matches mentions of the full iOS versions: “iOS 7”, “iOS 11” and “iOS 10”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total matches found: 3\n",
      "Match found: iOS 7\n",
      "Match found: iOS 11\n",
      "Match found: iOS 10\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "doc = nlp(\n",
    "    \"After making the iOS update you won't notice a radical system-wide \"\n",
    "    \"redesign: nothing like the aesthetic upheaval we got with iOS 7. Most of \"\n",
    "    \"iOS 11's furniture remains the same as in iOS 10. But you will discover \"\n",
    "    \"some tweaks once you delve a little deeper.\"\n",
    ")\n",
    "\n",
    "# Write a pattern for full iOS versions (\"iOS 7\", \"iOS 11\", \"iOS 10\")\n",
    "pattern = [{\"TEXT\": \"iOS\"}, {\"IS_DIGIT\": True}]\n",
    "\n",
    "# Add the pattern to the matcher and apply the matcher to the doc\n",
    "matcher.add(\"IOS_VERSION_PATTERN\", None, pattern)\n",
    "matches = matcher(doc)\n",
    "print(\"Total matches found:\", len(matches))\n",
    "\n",
    "# Iterate over the matches and print the span text\n",
    "for match_id, start, end in matches:\n",
    "    print(\"Match found:\", doc[start:end].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Write one pattern that only matches forms of “download” (tokens with the lemma “download”), followed by a token with the part-of-speech tag 'PROPN' (proper noun)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total matches found: 3\n",
      "Match found: downloaded Fortnite\n",
      "Match found: downloading Minecraft\n",
      "Match found: download Winzip\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\n",
    "    \"i downloaded Fortnite on my laptop and can't open the game at all. Help? \"\n",
    "    \"so when I was downloading Minecraft, I got the Windows version where it \"\n",
    "    \"is the '.zip' folder and I used the default program to unpack it... do \"\n",
    "    \"I also need to download Winzip?\"\n",
    ")\n",
    "\n",
    "# Write a pattern that matches a form of \"download\" plus proper noun\n",
    "pattern = [{\"LEMMA\": 'download'}, {\"POS\": 'PROPN'}]\n",
    "\n",
    "# Add the pattern to the matcher and apply the matcher to the doc\n",
    "matcher.add(\"DOWNLOAD_THINGS_PATTERN\", None, pattern)\n",
    "matches = matcher(doc)\n",
    "print(\"Total matches found:\", len(matches))\n",
    "\n",
    "# Iterate over the matches and print the span text\n",
    "for match_id, start, end in matches:\n",
    "    print(\"Match found:\", doc[start:end].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Write one pattern that matches adjectives ('ADJ') followed by one or two 'NOUN's (one noun and one optional noun)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total matches found: 4\n",
      "Match found: beautiful design\n",
      "Match found: smart search\n",
      "Match found: automatic labels\n",
      "Match found: optional voice responses\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\n",
    "    \"Features of the app include a beautiful design, smart search, automatic \"\n",
    "    \"labels and optional voice responses.\"\n",
    ")\n",
    "\n",
    "# Write a pattern for adjective plus one or two nouns\n",
    "pattern = [{\"POS\": \"ADJ\"}, {\"POS\": \"NOUN\"}, {\"POS\": \"NOUN\", \"OP\": \"?\"}]\n",
    "\n",
    "# Add the pattern to the matcher and apply the matcher to the doc\n",
    "matcher.add(\"ADJ_NOUN_PATTERN\", None, pattern)\n",
    "matches = matcher(doc)\n",
    "print(\"Total matches found:\", len(matches))\n",
    "\n",
    "# Iterate over the matches and print the span text\n",
    "for match_id, start, end in matches:\n",
    "    print(\"Match found:\", doc[start:end].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 2:  Large-scale data analysis with spaCy\n",
    "### Data Structures:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Strings to Hashes\n",
    "- Look up the string “cat” in nlp.vocab.strings to get the hash.\n",
    "- Look up the hash to get back the string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5439657043933447811\n",
      "cat\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\"I have a cat\")\n",
    "\n",
    "# Look up the hash for the word \"cat\"\n",
    "cat_hash = nlp.vocab.strings[\"cat\"]\n",
    "print(cat_hash)\n",
    "\n",
    "# Look up the cat_hash to get the string\n",
    "cat_string = nlp.vocab.strings[cat_hash]\n",
    "print(cat_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Look up the string label “PERSON” in nlp.vocab.strings to get the hash.\n",
    "- Look up the hash to get back the string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "380\n",
      "PERSON\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\"David Bowie is a PERSON\")\n",
    "\n",
    "# Look up the hash for the string label \"PERSON\"\n",
    "person_hash = nlp.vocab.strings[\"PERSON\"]\n",
    "print(person_hash)\n",
    "\n",
    "# Look up the person_hash to get the string\n",
    "person_string = nlp.vocab.strings[person_hash]\n",
    "print(person_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating a Doc\n",
    "- Import the Doc from spacy.tokens.\n",
    "- Create a Doc from the words and spaces. Don’t forget to pass in the vocab!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spaCy is cool!\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Import the Doc class\n",
    "from spacy.tokens import Doc\n",
    "\n",
    "# Desired text: \"spaCy is cool!\"\n",
    "words = [\"spaCy\", \"is\", \"cool\", \"!\"]\n",
    "spaces = [True, True, False, False]\n",
    "\n",
    "# Create a Doc from the words and spaces\n",
    "doc = Doc(nlp.vocab, words=words, spaces=spaces)\n",
    "print(doc.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Docs, Spans, and Entities From Scratch\n",
    "\n",
    "- Import the Doc and Span classes from spacy.tokens.\n",
    "- Use the Doc class directly to create a doc from the words and spaces.\n",
    "- Create a Span for “David Bowie” from the doc and assign it the label \"PERSON\".\n",
    "- Overwrite the doc.ents with a list of one entity, the “David Bowie” span."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I like David Bowie\n",
      "David Bowie PERSON\n",
      "[('David Bowie', 'PERSON')]\n"
     ]
    }
   ],
   "source": [
    "nlp = English()\n",
    "\n",
    "# Import the Doc and Span classes\n",
    "from spacy.tokens import Doc, Span\n",
    "\n",
    "words = [\"I\", \"like\", \"David\", \"Bowie\"]\n",
    "spaces = [True, True, True, False]\n",
    "\n",
    "# Create a doc from the words and spaces\n",
    "doc = Doc(nlp.vocab, words=words, spaces=spaces)\n",
    "print(doc.text)\n",
    "\n",
    "# Create a span for \"David Bowie\" from the doc and assign it the label \"PERSON\"\n",
    "span = Span(doc, 2, 4, label='PERSON')\n",
    "print(span.text, span.label_)\n",
    "\n",
    "# Add the span to the doc's entities\n",
    "doc.ents = [span]\n",
    "\n",
    "# Print entities' text and labels\n",
    "print([(ent.text, ent.label_) for ent in doc.ents])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Loop over each token in the doc and check the token.pos_ attribute.\n",
    "- Use doc[token.i + 1] to check for the next token and its .pos_ attribute.\n",
    "- If a proper noun before a verb is found, print its token.text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found proper noun before a verb: Berlin\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\"Berlin is a nice city\")\n",
    "\n",
    "# Iterate over the tokens\n",
    "for token in doc:\n",
    "    # Check if the current token is a proper noun\n",
    "    if token.pos_ == \"PROPN\":\n",
    "        # Check if the next token is a verb\n",
    "        if doc[token.i + 1].pos_ == \"VERB\":\n",
    "            print(\"Found proper noun before a verb:\", token.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Vectors and Semantic Similarities\n",
    "#### Inspecting Word Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-2.2009e-01 -3.0322e-02 -7.9859e-02 -4.6279e-01 -3.8600e-01  3.6962e-01\n",
      " -7.7178e-01 -1.1529e-01  3.3601e-02  5.6573e-01 -2.4001e-01  4.1833e-01\n",
      "  1.5049e-01  3.5621e-01 -2.1508e-01 -4.2743e-01  8.1400e-02  3.3916e-01\n",
      "  2.1637e-01  1.4792e-01  4.5811e-01  2.0966e-01 -3.5706e-01  2.3800e-01\n",
      "  2.7971e-02 -8.4538e-01  4.1917e-01 -3.9181e-01  4.0434e-04 -1.0662e+00\n",
      "  1.4591e-01  1.4643e-03  5.1277e-01  2.6072e-01  8.3785e-02  3.0340e-01\n",
      "  1.8579e-01  5.9999e-02 -4.0270e-01  5.0888e-01 -1.1358e-01 -2.8854e-01\n",
      " -2.7068e-01  1.1017e-02 -2.2217e-01  6.9076e-01  3.6459e-02  3.0394e-01\n",
      "  5.6989e-02  2.2733e-01 -9.9473e-02  1.5165e-01  1.3540e-01 -2.4965e-01\n",
      "  9.8078e-01 -8.0492e-01  1.9326e-01  3.1128e-01  5.5390e-02 -4.2423e-01\n",
      " -1.4082e-02  1.2708e-01  1.8868e-01  5.9777e-02 -2.2215e-01 -8.3950e-01\n",
      "  9.1987e-02  1.0180e-01 -3.1299e-01  5.5083e-01 -3.0717e-01  4.4201e-01\n",
      "  1.2666e-01  3.7643e-01  3.2333e-01  9.5673e-02  2.5083e-01 -6.4049e-02\n",
      "  4.2143e-01 -1.9375e-01  3.8026e-01  7.0883e-03 -2.0371e-01  1.5402e-01\n",
      " -3.7877e-03 -2.9396e-01  9.6518e-01  2.0068e-01 -5.6572e-01 -2.2581e-01\n",
      "  3.2251e-01 -3.4634e-01  2.7064e-01 -2.0687e-01 -4.7229e-01  3.1704e-01\n",
      " -3.4665e-01 -2.5188e-01 -1.1201e-01 -3.3937e-01  3.1518e-01 -3.2221e-01\n",
      " -2.4530e-01 -7.1571e-02 -4.3971e-01 -1.2070e+00  3.3365e-01 -5.8208e-02\n",
      "  8.0899e-01  4.2335e-01  3.8678e-01 -6.0797e-01 -7.3760e-01 -2.0547e-01\n",
      " -1.7499e-01 -3.7842e-03  2.1930e-01 -5.2486e-02  3.4869e-01  4.3852e-01\n",
      " -3.4471e-01  2.8910e-01  7.2554e-02 -4.8625e-01 -3.8390e-01 -4.4760e-01\n",
      "  4.3278e-01 -2.7128e-03 -9.0067e-01 -3.0819e-02 -3.8630e-01 -8.0798e-02\n",
      " -1.6243e-01  2.8830e-01 -2.6349e-01  1.7628e-01  3.5958e-01  5.7672e-01\n",
      " -5.4624e-01  3.8555e-02 -2.0182e+00  3.2916e-01  3.4672e-01  1.5398e-01\n",
      " -4.3446e-01 -4.1428e-02 -6.9588e-02  5.1513e-01 -1.3489e-01 -5.7239e-02\n",
      "  4.9241e-01  1.8643e-01  3.8596e-01 -3.7329e-02 -5.4216e-01 -1.8152e-01\n",
      "  4.3110e-01 -4.6967e-01  6.6801e-02  5.0323e-01 -2.4059e-01  3.6742e-01\n",
      "  2.9300e-01 -8.7883e-02 -4.7940e-01 -4.3431e-02 -2.6137e-01 -6.2658e-01\n",
      "  1.1446e-01  2.7682e-01  3.4800e-01  5.0018e-01  1.4269e-01 -3.3545e-01\n",
      " -3.9712e-01 -3.3121e-01 -3.4434e-01 -4.1627e-01 -3.5707e-03 -6.2350e-01\n",
      "  3.7794e-01 -1.6765e-01 -4.1954e-01 -3.3134e-01  3.1232e-01 -3.9494e-01\n",
      " -4.6921e-03 -4.8884e-01 -2.2059e-02 -2.6174e-01  1.7937e-01  3.6628e-01\n",
      "  5.8971e-02 -3.5991e-01 -4.4393e-01 -1.1890e-01  3.3487e-01  3.6505e-02\n",
      " -3.2788e-01  3.3425e-01 -5.6361e-01 -1.1190e-01  5.3770e-01  2.0311e-01\n",
      "  1.5110e-01  1.0623e-02  3.3401e-01  4.6084e-01  5.6293e-01 -7.5432e-02\n",
      "  5.4813e-01  1.9395e-01 -2.6265e-01 -3.1699e-01 -8.1778e-01  5.8169e-02\n",
      " -5.7866e-02 -1.1781e-01 -5.8742e-02 -1.4092e-01 -9.9394e-01 -9.4532e-02\n",
      "  2.3503e-01 -4.9027e-01  8.5832e-01  1.1540e-01 -1.5049e-01  1.9065e-01\n",
      " -2.6705e-01  2.5326e-01 -6.7579e-01 -1.0633e-02 -5.5158e-02 -3.1004e-01\n",
      " -5.8036e-02 -1.7200e-01  1.3298e-01 -3.2899e-01 -7.5481e-02  2.9425e-02\n",
      " -3.2949e-01 -1.8691e-01 -9.5323e-01 -3.5468e-01 -3.3162e-01  5.6441e-02\n",
      "  2.1790e-02  1.7182e-01 -4.4267e-01  6.9765e-01 -2.6876e-01  1.1659e-01\n",
      " -1.6584e-01  3.8296e-01  2.9109e-01  3.6318e-01  3.6961e-01  1.6305e-01\n",
      "  1.8152e-01  2.2453e-01  3.9866e-02 -3.7607e-02 -3.6089e-01  7.0818e-02\n",
      " -2.1509e-01  3.6551e-01 -5.1603e-01 -5.8102e-03 -4.8320e-01 -2.5068e-01\n",
      " -5.2062e-02 -2.0828e-01  2.9060e-01  2.2084e-02 -6.8123e-01  4.2063e-01\n",
      "  9.5973e-02  8.1720e-01 -1.5241e-01  6.2994e-01  2.6449e-01 -1.3516e-01\n",
      "  3.2450e-01  3.0503e-01  1.2357e-01  1.5107e-01  2.8327e-01 -3.3838e-01\n",
      "  4.6106e-02 -1.2361e-01  1.4516e-01 -2.7947e-02  2.6231e-02 -5.9591e-01\n",
      " -4.4183e-01  7.8440e-01 -3.4375e-02 -1.3928e+00  3.5248e-01  6.5220e-01]\n"
     ]
    }
   ],
   "source": [
    "# Load the en_core_web_md model\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "\n",
    "# Process a text\n",
    "doc = nlp(\"Two bananas in pyjamas\")\n",
    "\n",
    "# Get the vector for the token \"bananas\"\n",
    "bananas_vector = doc[1].vector\n",
    "print(bananas_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comparing Similarities\n",
    "\n",
    "- Use the doc.similarity method to compare doc1 to doc2 and print the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8789265574516525\n"
     ]
    }
   ],
   "source": [
    "doc1 = nlp(\"It's a warm summer day\")\n",
    "doc2 = nlp(\"It's sunny outside\")\n",
    "\n",
    "# Get the similarity of doc1 and doc2\n",
    "similarity = doc1.similarity(doc2)\n",
    "print(similarity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Use the token.similarity method to compare token1 to token2 and print the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.22325331\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"TV and books\")\n",
    "token1, token2 = doc[0], doc[2]\n",
    "\n",
    "# Get the similarity of the tokens \"TV\" and \"books\"\n",
    "similarity = token1.similarity(token2)\n",
    "print(similarity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Create spans for “great restaurant”/“really nice bar”.\n",
    "- Use span.similarity to compare them and print the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.75173926\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"This was a great restaurant. Afterwards, we went to a really nice bar.\")\n",
    "\n",
    "# Create spans for \"great restaurant\" and \"really nice bar\"\n",
    "span1 = doc[3:5]\n",
    "span2 = doc[12:15]\n",
    "\n",
    "# Get the similarity of the spans\n",
    "similarity = span1.similarity(span2)\n",
    "print(similarity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining Models and Rules:\n",
    "\n",
    "#### Debugging Patterns\n",
    "\n",
    "- Edit pattern1 so that it correctly matches all case-insensitive mentions of \"Amazon\" plus a title-cased proper noun.\n",
    "- Edit pattern2 so that it correctly matches all case-insensitive mentions of \"ad-free\", plus the following noun."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PATTERN1 Amazon Prime\n",
      "PATTERN2 ad-free viewing\n",
      "PATTERN1 Amazon Prime\n",
      "PATTERN2 ad-free viewing\n",
      "PATTERN2 ad-free viewing\n",
      "PATTERN2 ad-free viewing\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\n",
    "    \"Twitch Prime, the perks program for Amazon Prime members offering free \"\n",
    "    \"loot, games and other benefits, is ditching one of its best features: \"\n",
    "    \"ad-free viewing. According to an email sent out to Amazon Prime members \"\n",
    "    \"today, ad-free viewing will no longer be included as a part of Twitch \"\n",
    "    \"Prime for new members, beginning on September 14. However, members with \"\n",
    "    \"existing annual subscriptions will be able to continue to enjoy ad-free \"\n",
    "    \"viewing until their subscription comes up for renewal. Those with \"\n",
    "    \"monthly subscriptions will have access to ad-free viewing until October 15.\"\n",
    ")\n",
    "\n",
    "# Create the match patterns\n",
    "pattern1 = [{\"LOWER\": \"amazon\"}, {\"IS_TITLE\": True, \"POS\": \"PROPN\"}]\n",
    "pattern2 = [{\"LOWER\": \"ad\"}, {\"TEXT\": \"-\"}, {\"LOWER\": \"free\"}, {\"POS\": \"NOUN\"}]\n",
    "\n",
    "# Initialize the Matcher and add the patterns\n",
    "matcher = Matcher(nlp.vocab)\n",
    "matcher.add(\"PATTERN1\", None, pattern1)\n",
    "matcher.add(\"PATTERN2\", None, pattern2)\n",
    "\n",
    "# Iterate over the matches\n",
    "for match_id, start, end in matcher(doc):\n",
    "    # Print pattern string name and text of matched span\n",
    "    print(doc.vocab.strings[match_id], doc[start:end].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Efficient Phrase Matching\n",
    "\n",
    "- Import the PhraseMatcher and initialize it with the shared vocab as the variable matcher.\n",
    "- Add the phrase patterns and call the matcher on the doc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Czech Republic, Slovakia]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from spacy.lang.en import English\n",
    "\n",
    "nlp = English()\n",
    "doc = nlp(\"Czech Republic may help Slovakia protect its airspace\")\n",
    "\n",
    "# Import the PhraseMatcher and initialize it\n",
    "from spacy.matcher import PhraseMatcher\n",
    "\n",
    "matcher = PhraseMatcher(nlp.vocab)\n",
    "\n",
    "# Create pattern Doc objects and add them to the matcher\n",
    "# This is the faster version of: [nlp(country) for country in COUNTRIES]\n",
    "patterns = list(nlp.pipe(COUNTRIES))\n",
    "matcher.add(\"COUNTRY\", None, *patterns)\n",
    "\n",
    "# Call the matcher on the test document and print the result\n",
    "matches = matcher(doc)\n",
    "print([doc[start:end] for match_id, start, end in matches])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extracting Countries and Relationships\n",
    "\n",
    "- Iterate over the matches and create a Span with the label \"GPE\" (geopolitical entity).\n",
    "- Overwrite the entities in doc.ents and add the matched span.\n",
    "- Get the matched span’s root head token.\n",
    "- Print the text of the head token and the span."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namibia --> Namibia\n",
      "South --> South Africa\n",
      "Cambodia --> Cambodia\n",
      "Kuwait --> Kuwait\n",
      "Somalia --> Somalia\n",
      "Haiti --> Haiti\n",
      "Mozambique --> Mozambique\n",
      "Somalia --> Somalia\n",
      "Rwanda --> Rwanda\n",
      "Singapore --> Singapore\n",
      "Sierra --> Sierra Leone\n",
      "Afghanistan --> Afghanistan\n",
      "Iraq --> Iraq\n",
      "Sudan --> Sudan\n",
      "Congo --> Congo\n",
      "Haiti --> Haiti\n",
      "[('Namibia', 'GPE'), ('South Africa', 'GPE'), ('Cambodia', 'GPE'), ('Kuwait', 'GPE'), ('Somalia', 'GPE'), ('Haiti', 'GPE'), ('Mozambique', 'GPE'), ('Somalia', 'GPE'), ('Rwanda', 'GPE'), ('Singapore', 'GPE'), ('Sierra Leone', 'GPE'), ('Afghanistan', 'GPE'), ('Iraq', 'GPE'), ('Sudan', 'GPE'), ('Congo', 'GPE'), ('Haiti', 'GPE')]\n"
     ]
    }
   ],
   "source": [
    "nlp = English()\n",
    "matcher = PhraseMatcher(nlp.vocab)\n",
    "patterns = list(nlp.pipe(COUNTRIES))\n",
    "matcher.add(\"COUNTRY\", None, *patterns)\n",
    "\n",
    "# Create a doc and find matches in it\n",
    "doc = nlp(COUNTRY_TEXT)\n",
    "\n",
    "# Iterate over the matches\n",
    "for match_id, start, end in matcher(doc):\n",
    "    # Create a Span with the label for \"GPE\"\n",
    "    span = Span(doc, start, end, label=\"GPE\")\n",
    "\n",
    "    # Overwrite the doc.ents and add the span\n",
    "    doc.ents = list(doc.ents) + [span]\n",
    "\n",
    "    # Get the span's root head token\n",
    "    span_root_head = span.root.head\n",
    "    # Print the text of the span root's head token and the span text\n",
    "    print(span_root_head.text, \"-->\", span.text)\n",
    "\n",
    "# Print the entities in the document\n",
    "print([(ent.text, ent.label_) for ent in doc.ents if ent.label_ == \"GPE\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 3: Processing Pipelines\n",
    "\n",
    "#### Inspecting the Pipeline\n",
    "\n",
    "- Load the en_core_web_sm model and create the nlp object.\n",
    "- Print the names of the pipeline components using nlp.pipe_names.\n",
    "- Print the full pipeline of (name, component) tuples using nlp.pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tagger', 'parser', 'ner']\n",
      "[('tagger', <spacy.pipeline.pipes.Tagger object at 0x7f8044bed3c8>), ('parser', <spacy.pipeline.pipes.DependencyParser object at 0x7f8044869be8>), ('ner', <spacy.pipeline.pipes.EntityRecognizer object at 0x7f8044869d68>)]\n"
     ]
    }
   ],
   "source": [
    "# Load the en_core_web_sm model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Print the names of the pipeline components\n",
    "print(nlp.pipe_names)\n",
    "\n",
    "# Print the full pipeline of (name, component) tuples\n",
    "print(nlp.pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Simple Components (customizable)\n",
    "\n",
    "- Complete the component function with the doc’s length.\n",
    "- Add the length_component to the existing pipeline as the first component.\n",
    "- Try out the new pipeline and process any text with the nlp object – for example “This is a sentence.”.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['length_component', 'tagger', 'parser', 'ner']\n",
      "This document is 5 tokens long.\n"
     ]
    }
   ],
   "source": [
    "# Define the custom component\n",
    "def length_component(doc):\n",
    "    # Get the doc's length\n",
    "    doc_length = len(doc)\n",
    "    print(\"This document is {} tokens long.\".format(doc_length))\n",
    "    # Return the doc\n",
    "    return doc\n",
    "\n",
    "\n",
    "# Load the small English model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Add the component first in the pipeline and print the pipe names\n",
    "nlp.add_pipe(length_component, first=True)\n",
    "print(nlp.pipe_names)\n",
    "\n",
    "# Process a text\n",
    "doc = nlp('This is a sentence.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Complex Components\n",
    "- Define the custom component and apply the matcher to the doc.\n",
    "- Create a Span for each match, assign the label ID for 'ANIMAL' and overwrite the doc.ents with the new spans.\n",
    "- Add the new component to the pipeline after the 'ner' component.\n",
    "- Process the text and print the entity text and entity label for the entities in doc.ents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "animal_patterns: [Golden Retriever, cat, turtle, Rattus norvegicus]\n",
      "['tagger', 'parser', 'ner', 'animal_component']\n",
      "[('cat', 'ANIMAL'), ('Golden Retriever', 'ANIMAL')]\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "animals = [\"Golden Retriever\", \"cat\", \"turtle\", \"Rattus norvegicus\"]\n",
    "animal_patterns = list(nlp.pipe(animals))\n",
    "print(\"animal_patterns:\", animal_patterns)\n",
    "matcher = PhraseMatcher(nlp.vocab)\n",
    "matcher.add(\"ANIMAL\", None, *animal_patterns)\n",
    "\n",
    "# Define the custom component\n",
    "def animal_component(doc):\n",
    "    # Apply the matcher to the doc\n",
    "    matches = matcher(doc)\n",
    "    # Create a Span for each match and assign the label 'ANIMAL'\n",
    "    spans = [Span(doc, start, end, label=\"ANIMAL\") for match_id, start, end in matches]\n",
    "    # Overwrite the doc.ents with the matched spans\n",
    "    doc.ents = spans\n",
    "    return doc\n",
    "\n",
    "\n",
    "# Add the component to the pipeline after the 'ner' component\n",
    "nlp.add_pipe(animal_component, after=\"ner\")\n",
    "print(nlp.pipe_names)\n",
    "\n",
    "# Process the text and print the text and label for the doc.ents\n",
    "doc = nlp(\"I have a cat and a Golden Retriever\")\n",
    "print([(ent.text, ent.label_) for ent in doc.ents])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setting Extension Attributes\n",
    "- Use Token.set_extension to register is_country (default False).\n",
    "- Update it for \"Spain\" and print it for all tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('I', False), ('live', False), ('in', False), ('Spain', True), ('.', False)]\n"
     ]
    }
   ],
   "source": [
    "from spacy.tokens import Token\n",
    "\n",
    "nlp = English()\n",
    "\n",
    "# Register the Token extension attribute 'is_country' with the default value False\n",
    "Token.set_extension('is_country', default=False, force=True)\n",
    "\n",
    "# Process the text and set the is_country attribute to True for the token \"Spain\"\n",
    "doc = nlp(\"I live in Spain.\")\n",
    "doc[3]._.is_country = True\n",
    "\n",
    "# Print the token text and the is_country attribute for all tokens\n",
    "print([(token.text, token._.is_country) for token in doc])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Use Token.set_extension to register 'reversed' (getter function get_reversed).\n",
    "- Print its value for each token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reversed: llA\n",
      "reversed: snoitazilareneg\n",
      "reversed: era\n",
      "reversed: eslaf\n",
      "reversed: ,\n",
      "reversed: gnidulcni\n",
      "reversed: siht\n",
      "reversed: eno\n",
      "reversed: .\n"
     ]
    }
   ],
   "source": [
    "# Define the getter function that takes a token and returns its reversed text\n",
    "def get_reversed(token):\n",
    "    return token.text[::-1]\n",
    "\n",
    "# Register the Token property extension 'reversed' with the getter get_reversed\n",
    "Token.set_extension(\"reversed\", getter=get_reversed, force=True)\n",
    "\n",
    "# Process the text and print the reversed attribute for each token\n",
    "doc = nlp(\"All generalizations are false, including this one.\")\n",
    "for token in doc:\n",
    "    print(\"reversed:\", token._.reversed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Complete the has_number function .\n",
    "- Use Doc.set_extension to register has_number (getter get_has_number) and print its value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "has_number: True\n"
     ]
    }
   ],
   "source": [
    "# Define the getter function\n",
    "def get_has_number(doc):\n",
    "    # Return if any of the tokens in the doc return True for token.like_num\n",
    "    return any(token.like_num for token in doc)\n",
    "\n",
    "\n",
    "# Register the Doc property extension 'has_number' with the getter get_has_number\n",
    "Doc.set_extension('has_number', getter=get_has_number,force=True)\n",
    "\n",
    "# Process the text and check the custom has_number attribute\n",
    "doc = nlp(\"The museum closed for five years in 2012.\")\n",
    "print(\"has_number:\", doc._.has_number)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Use Span.set_extension to register 'to_html' (method to_html).\n",
    "- Call it on doc[0:2] with the tag 'strong'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<strong>Hello world</strong>\n"
     ]
    }
   ],
   "source": [
    "nlp = English()\n",
    "\n",
    "# Define the method\n",
    "def to_html(span, tag):\n",
    "    # Wrap the span text in a HTML tag and return it\n",
    "    return \"<{tag}>{text}</{tag}>\".format(tag=tag, text=span.text)\n",
    "\n",
    "\n",
    "# Register the Span property extension 'to_html' with the method to_html\n",
    "Span.set_extension(\"to_html\", method=to_html)\n",
    "\n",
    "# Process the text and call the to_html method on the span with the tag name 'strong'\n",
    "doc = nlp(\"Hello world, this is a sentence.\")\n",
    "span = doc[0:2]\n",
    "print(span._.to_html(\"strong\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Entities and extensions\n",
    "- Complete the get_wikipedia_url getter so it only returns the URL if the span’s label is in the list of labels.\n",
    "- Set the Span extension 'wikipedia_url' using the getter get_wikipedia_url.\n",
    "- Iterate over the entities in the doc and output their Wikipedia URL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "over fifty years None\n",
      "first None\n",
      "David Bowie https://en.wikipedia.org/w/index.php?search=David_Bowie\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "\n",
    "def get_wikipedia_url(span):\n",
    "    # Get a Wikipedia URL if the span has one of the labels\n",
    "    if span.label_ in (\"PERSON\", \"ORG\", \"GPE\", \"LOCATION\"):\n",
    "        entity_text = span.text.replace(\" \", \"_\")\n",
    "        return \"https://en.wikipedia.org/w/index.php?search=\" + entity_text\n",
    "\n",
    "\n",
    "# Set the Span extension wikipedia_url using get getter get_wikipedia_url\n",
    "Span.set_extension('wikipedia_url', getter=get_wikipedia_url)\n",
    "\n",
    "doc = nlp(\n",
    "    \"In over fifty years from his very first recordings right through to his \"\n",
    "    \"last album, David Bowie was at the vanguard of contemporary culture.\"\n",
    ")\n",
    "for ent in doc.ents:\n",
    "    # Print the text and Wikipedia URL of the entity\n",
    "    print(ent.text, ent._.wikipedia_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Components with extensions\n",
    "- Complete the countries_component and create a Span with the label 'GPE' (geopolitical entity) for all matches.\n",
    "- Add the component to the pipeline.\n",
    "- Register the Span extension attribute 'capital' with the getter get_capital.\n",
    "- Process the text and print the entity text, entity label and entity capital for each entity span in doc.ents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['countries_component']\n",
      "[('Czech Republic', 'GPE', 'Prague'), ('Slovakia', 'GPE', 'Bratislava')]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from spacy.lang.en import English\n",
    "from spacy.tokens import Span\n",
    "from spacy.matcher import PhraseMatcher\n",
    "\n",
    "nlp = English()\n",
    "matcher = PhraseMatcher(nlp.vocab)\n",
    "matcher.add(\"COUNTRY\", None, *list(nlp.pipe(COUNTRIES)))\n",
    "\n",
    "\n",
    "def countries_component(doc):\n",
    "    # Create an entity Span with the label 'GPE' for all matches\n",
    "    matches = matcher(doc)\n",
    "    doc.ents = [Span(doc, start, end, label='GPE') for match_id, start, end in matches]\n",
    "    return doc\n",
    "\n",
    "\n",
    "# Add the component to the pipeline\n",
    "nlp.add_pipe(countries_component)\n",
    "print(nlp.pipe_names)\n",
    "\n",
    "# Getter that looks up the span text in the dictionary of country capitals\n",
    "get_capital = lambda span: CAPITALS.get(span.text)\n",
    "\n",
    "# Register the Span extension attribute 'capital' with the getter get_capital\n",
    "Span.set_extension('capital', getter=get_capital)\n",
    "\n",
    "# Process the text and print the entity text, label and capital attributes\n",
    "doc = nlp(\"Czech Republic may help Slovakia protect its airspace\")\n",
    "print([(ent.text, ent.label_, ent._.capital) for ent in doc.ents])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Processing Streams\n",
    "- Rewrite the example to use nlp.pipe. Instead of iterating over the texts and processing them, iterate over the doc objects yielded by nlp.pipe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'exercises/tweets.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-3acc32542aba>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mnlp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"en_core_web_sm\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"exercises/tweets.json\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mTEXTS\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'exercises/tweets.json'"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "with open(\"exercises/tweets.json\") as f:\n",
    "    TEXTS = json.loads(f.read())\n",
    "\n",
    "# Process the texts and print the adjectives\n",
    "'''\n",
    "for text in TEXTS:\n",
    "    doc = nlp(text)\n",
    "    print([token.text for token in doc if token.pos_ == \"ADJ\"])\n",
    "'''\n",
    "#re-written\n",
    "for doc in nlp.pipe(TEXTS):\n",
    "    print([token.text for token in doc if token.pos_ == \"ADJ\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Rewrite the example to use nlp.pipe. Don’t forget to call list() around the result to turn it into a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'exercises/tweets.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-41799201dc5a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"exercises/tweets.json\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mTEXTS\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Process the texts and print the entities\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m '''docs = [nlp(text) for text in TEXTS]\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'exercises/tweets.json'"
     ]
    }
   ],
   "source": [
    "with open(\"exercises/tweets.json\") as f:\n",
    "    TEXTS = json.loads(f.read())\n",
    "\n",
    "# Process the texts and print the entities\n",
    "'''docs = [nlp(text) for text in TEXTS]\n",
    "entities = [doc.ents for doc in docs]\n",
    "print(*entities)\n",
    "'''\n",
    "#re-written\n",
    "docs = list(nlp.pipe(TEXTS))\n",
    "entities = [doc.ents for doc in docs]\n",
    "print(*entities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Rewrite the example to use nlp.pipe. Don’t forget to call list() around the result to turn it into a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = English()\n",
    "\n",
    "people = [\"David Bowie\", \"Angela Merkel\", \"Lady Gaga\"]\n",
    "\n",
    "'''\n",
    "# Create a list of patterns for the PhraseMatcher\n",
    "patterns = [nlp(person) for person in people]\n",
    "'''\n",
    "#re-written\n",
    "patterns = list(nlp.pipe(people))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Processing data with context\n",
    "- Use the set_extension method to register the custom attributes 'author' and 'book' on the Doc, which default to None.\n",
    "- Process the [text, context] pairs in DATA using nlp.pipe with as_tuples=True.\n",
    "- Overwrite the doc._.book and doc._.author with the respective info passed in as the context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One morning, when Gregor Samsa woke from troubled dreams, he found himself transformed in his bed into a horrible vermin. \n",
      " — 'Metamorphosis' by Franz Kafka \n",
      "\n",
      "I know not all that may be coming, but be it what it will, I'll go to it laughing. \n",
      " — 'Moby-Dick or, The Whale' by Herman Melville \n",
      "\n",
      "It was the best of times, it was the worst of times. \n",
      " — 'A Tale of Two Cities' by Charles Dickens \n",
      "\n",
      "The only people for me are the mad ones, the ones who are mad to live, mad to talk, mad to be saved, desirous of everything at the same time, the ones who never yawn or say a commonplace thing, but burn, burn, burn like fabulous yellow roman candles exploding like spiders across the stars. \n",
      " — 'On the Road' by Jack Kerouac \n",
      "\n",
      "It was a bright cold day in April, and the clocks were striking thirteen. \n",
      " — '1984' by George Orwell \n",
      "\n",
      "Nowadays people know the price of everything and the value of nothing. \n",
      " — 'The Picture Of Dorian Gray' by Oscar Wilde \n",
      "\n"
     ]
    }
   ],
   "source": [
    "nlp = English()\n",
    "\n",
    "# Register the Doc extension 'author' (default None)\n",
    "Doc.set_extension(\"author\", default=None)\n",
    "\n",
    "# Register the Doc extension 'book' (default None)\n",
    "Doc.set_extension(\"book\", default=None)\n",
    "\n",
    "for doc, context in nlp.pipe(DATA, as_tuples=True):\n",
    "    # Set the doc._.book and doc._.author attributes from the context\n",
    "    doc._.book = context[\"book\"]\n",
    "    doc._.author = context[\"author\"]\n",
    "\n",
    "    # Print the text and custom attribute data\n",
    "    print(doc.text, \"\\n\", \"— '{}' by {}\".format(doc._.book, doc._.author), \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Selective Processing\n",
    "\n",
    "- Rewrite the code to only tokenize the text using nlp.make_doc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Chick', '-', 'fil', '-', 'A', 'is', 'an', 'American', 'fast', 'food', 'restaurant', 'chain', 'headquartered', 'in', 'the', 'city', 'of', 'College', 'Park', ',', 'Georgia', ',', 'specializing', 'in', 'chicken', 'sandwiches', '.']\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "text = (\n",
    "    \"Chick-fil-A is an American fast food restaurant chain headquartered in \"\n",
    "    \"the city of College Park, Georgia, specializing in chicken sandwiches.\"\n",
    ")\n",
    "'''\n",
    "# Only tokenize the text\n",
    "doc = nlp(text)\n",
    "print([token.text for token in doc])\n",
    "'''\n",
    "#re-written\n",
    "doc = nlp.make_doc(text)\n",
    "print([token.text for token in doc])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Disable the tagger and parser using the nlp.disable_pipes method.\n",
    "- Process the text and print all entities in the doc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(American, College Park, Georgia)\n"
     ]
    }
   ],
   "source": [
    "# Disable tagger and parser\n",
    "with nlp.disable_pipes('tagger', 'parser'):\n",
    "    # Process the text and print the entities\n",
    "    doc = nlp(text)\n",
    "    print(doc.ents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
